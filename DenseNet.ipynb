{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from  matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10_Shikara',\n",
       " '11_Kapitta',\n",
       " '12_Katakamukhaha',\n",
       " '13_Soochi',\n",
       " '14_Chandrakala',\n",
       " '15_Padmakosha',\n",
       " '16_Sarpasheeshaha',\n",
       " '17_Mrughasheeha',\n",
       " '18_Sihamukhaha',\n",
       " '19_Kangoola',\n",
       " '1_Pataka',\n",
       " '20_Alapadma',\n",
       " '21_Chatura',\n",
       " '22_Bramhara',\n",
       " '23_Hamsasya',\n",
       " '24_Hamsapashakaha',\n",
       " '25_Shandamshu',\n",
       " '26_Samadamaso',\n",
       " '27_Mukula',\n",
       " '28_Tamrachooda',\n",
       " '29_Trishoola',\n",
       " '2_Tripataka',\n",
       " '3_Ardhapataka',\n",
       " '4_Kartarimukhaha',\n",
       " '5_Mayura',\n",
       " '6_Ardhachandra',\n",
       " '7_Arala',\n",
       " '8_Shukatundakaha',\n",
       " '9_Mushti']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "MAIN_DIR = \"D:\\\\College_Semesters\\\\7th Semester\\\\F. Final\\\\Codes\\\\Datasets\\\\Initial_AugmentedDataset\"\n",
    "SEED = 40\n",
    "os.listdir(MAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(folder):\n",
    "    imgs = []\n",
    "    target = 0\n",
    "    labels = []\n",
    "    for i in os.listdir(folder):\n",
    "        subdir = os.path.join(folder, i)\n",
    "        for j in os.listdir(subdir):\n",
    "            img_dir = os.path.join(subdir,j)\n",
    "            try:\n",
    "                img = cv2.imread(img_dir)\n",
    "                if img.shape[-1] == 1:\n",
    "                    # convert grayscale image to RGB\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                img = cv2.resize(img,(224,224))\n",
    "                imgs.append(img)\n",
    "                labels.append(target)\n",
    "            except:\n",
    "                continue\n",
    "        target += 1\n",
    "    \n",
    "    imgs = np.array(imgs)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = load_images(MAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Resize images\n",
    "new_width, new_height = 128, 128  # Adjust according to your requirements\n",
    "resized_data = [cv2.resize(img, (new_width, new_height)) for img in data]\n",
    "norm_data = np.array(resized_data) / 255.0\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "y = encoder.transform(labels)\n",
    "y_one_hot = to_categorical(y, num_classes=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (3480, 224, 224, 3)\n",
      "Train Labels Shape: (3480,)\n",
      "Test Data Shape: (870, 224, 224, 3)\n",
      "Test Labels Shape: (870,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the split datasets\n",
    "print(\"Train Data Shape:\", train_data.shape)\n",
    "print(\"Train Labels Shape:\", train_labels.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)\n",
    "print(\"Test Labels Shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert target labels to one-hot encoded format\n",
    "train_labels = to_categorical(train_labels, num_classes=29)\n",
    "test_labels = to_categorical(test_labels, num_classes=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, Concatenate, GlobalAveragePooling2D, Dense\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Assuming new_width and new_height are defined\n",
    "# input_shape = (224, 224, 3)\n",
    "\n",
    "# def dense_block(x, num_blocks, growth_rate):\n",
    "#     for _ in range(num_blocks):\n",
    "#         y = BatchNormalization()(x)\n",
    "#         y = ReLU()(y)\n",
    "#         y = Conv2D(growth_rate, kernel_size=(3, 3), padding='same')(y)\n",
    "#         x = Concatenate()([x, y])\n",
    "#     return x\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# # Initial Convolution layer\n",
    "# model.add(Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same', input_shape=input_shape))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(ReLU())\n",
    "\n",
    "# # First dense block\n",
    "# x = model.layers[-1].output  # Save the output of the previous layer\n",
    "# model.add(dense_block(x, num_blocks=6, growth_rate=32))\n",
    "\n",
    "# # Transition layer\n",
    "# model.add(Conv2D(128, kernel_size=(1, 1), strides=(2, 2), padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(ReLU())\n",
    "\n",
    "# # Second dense block\n",
    "# x = model.layers[-1].output\n",
    "# model.add(dense_block(x, num_blocks=12, growth_rate=32))\n",
    "\n",
    "# # Transition layer\n",
    "# model.add(Conv2D(256, kernel_size=(1, 1), strides=(2, 2), padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(ReLU())\n",
    "\n",
    "# # Third dense block\n",
    "# x = model.layers[-1].output\n",
    "# model.add(dense_block(x, num_blocks=24, growth_rate=32))\n",
    "\n",
    "# # Transition layer\n",
    "# model.add(Conv2D(512, kernel_size=(1, 1), strides=(2, 2), padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(ReLU())\n",
    "\n",
    "# # Global Average Pooling\n",
    "# model.add(GlobalAveragePooling2D())\n",
    "\n",
    "# # Fully connected layer\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dense(29, activation='softmax'))  # Adjust the number of units based on your number of classes (29)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Define early stopping callback\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, min_delta=0.001)\n",
    "\n",
    "# # Fit the model with early stopping\n",
    "# history = model.fit(train_data, train_labels, epochs=50, batch_size=32, \n",
    "#                     validation_data=(test_data, test_labels),\n",
    "#                     callbacks=[early_stopping])\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Concatenate, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def dense_block(x, num_blocks, growth_rate):\n",
    "    for _ in range(num_blocks):\n",
    "        y = BatchNormalization()(x)\n",
    "        y = ReLU()(y)\n",
    "        y = Conv2D(growth_rate, kernel_size=(3, 3), padding='same')(y)\n",
    "        x = Concatenate()([x, y])\n",
    "    return x\n",
    "\n",
    "def transition_layer(x, num_filters):\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(num_filters, kernel_size=(1, 1), strides=(2, 2), padding='same')(x)\n",
    "    return x\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Initial Convolution layer\n",
    "x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "# Dense Blocks and Transition Layers\n",
    "x = dense_block(x, num_blocks=6, growth_rate=32)\n",
    "x = transition_layer(x, num_filters=128)\n",
    "\n",
    "x = dense_block(x, num_blocks=12, growth_rate=32)\n",
    "x = transition_layer(x, num_filters=256)\n",
    "\n",
    "x = dense_block(x, num_blocks=24, growth_rate=32)\n",
    "x = transition_layer(x, num_filters=512)\n",
    "\n",
    "# Final dense block without a following transition layer\n",
    "x = dense_block(x, num_blocks=16, growth_rate=32)\n",
    "\n",
    "# Global Average Pooling and Fully connected layers\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "outputs = Dense(29, activation='softmax')(x)  # Adjust the number of units based on your number of classes (29)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, min_delta=0.001)\n",
    "\n",
    "# Fit the model with early stopping\n",
    "# Replace 'train_data', 'train_labels', 'test_data', 'test_labels' with your actual data\n",
    "history = model.fit(train_data, train_labels, epochs=50, batch_size=32, \n",
    "                    validation_data=(test_data, test_labels),\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"DenseNet\\\\DenseNet_New_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "predictions = model.predict(test_data)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print('Test Loss:', test_loss)\n",
    "\n",
    "accuracy = np.mean(predicted_labels == true_labels)\n",
    "print('Accuracy:', accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(test_data)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
